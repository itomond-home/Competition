{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import TargetEncoder, OneHotEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: oof: File exists\n",
      "mkdir: models: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir oof\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    VER = 1\n",
    "    DATA_PATH = Path('.')\n",
    "    OOF_DATA_PATH = Path('./oof')\n",
    "    MODEL_DATA_PATH = Path('./models')\n",
    "    METHOD_LIST = ['lightgbm', 'xgboost', 'catboost']\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target_col = 'attendance'\n",
    "    USE_PLAYER_FEATURES = False\n",
    "    metric_maximize_flag = False\n",
    "    num_boost_round = 50500\n",
    "    early_stopping_round = 500\n",
    "    verbose = 2000\n",
    "    boosting_type = 'gbdt' # 'dart'\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'l2',\n",
    "        'boosting': boosting_type,\n",
    "        'learning_rate': 0.005,\n",
    "        'num_leaves': 5,\n",
    "        'feature_fraction': 0.50,\n",
    "        'bagging_fraction': 0.80,\n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 4,\n",
    "        'n_jobs': -1,\n",
    "        'min_data_in_leaf': 40,\n",
    "        'bagging_freq': 10,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    xgb_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'learning_rate': 0.005,\n",
    "        'max_depth': 4,\n",
    "        'colsample_bytree': 0.50,\n",
    "        'subsample': 0.80,\n",
    "        'eta': 0.03,\n",
    "        'gamma': 1.5,\n",
    "        'lambda': 70,\n",
    "        'min_child_weight': 8,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "\n",
    "    cat_params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'learning_rate': 0.005,\n",
    "        'iterations': num_boost_round,\n",
    "        'depth': 4,\n",
    "        'colsample_bylevel': 0.50,\n",
    "        'subsample': 0.80,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'random_seed': seed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'rmse', np.sqrt(mean_squared_error(y_true, y_pred)), False\n",
    "\n",
    "def xgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'rmse', np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# ====================================================\n",
    "# Catboost Metric\n",
    "# ====================================================\n",
    "class CatboostMetric(object):\n",
    "    def get_final_error(self, error, weight): return error\n",
    "    def is_max_optimal(self): return False\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        error = np.sqrt(mean_squared_error(np.array(target), approxes))\n",
    "        return error, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # OneHot encode categorical features\n",
    "    one_hot_enc = OneHotEncoder(cols=categorical_features, handle_unknown='indicator')\n",
    "    x_train = one_hot_enc.fit_transform(x_train)\n",
    "    x_valid = one_hot_enc.transform(x_valid)\n",
    "\n",
    "    # Initialize target encoder and imputer\n",
    "    target_enc = TargetEncoder(cols=x_train.columns, handle_unknown='value', handle_missing='value')\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "\n",
    "    # Apply target encoding\n",
    "    x_train = target_enc.fit_transform(x_train, y_train)\n",
    "    x_valid = target_enc.transform(x_valid)\n",
    "    x_train = imputer.fit_transform(x_train)\n",
    "    x_valid = imputer.transform(x_valid)\n",
    "\n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(x_valid, label=y_valid)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(CFG.lgb_params, train_data, num_boost_round=CFG.num_boost_round, valid_sets=[valid_data], early_stopping_rounds=CFG.early_stopping_round, verbose_eval=CFG.verbose)\n",
    "\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred, one_hot_enc, target_enc,\n",
    "\n",
    "def xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # OneHot encode categorical features\n",
    "    one_hot_enc = OneHotEncoder(cols=categorical_features, handle_unknown='indicator')\n",
    "    x_train = one_hot_enc.fit_transform(x_train)\n",
    "    x_valid = one_hot_enc.transform(x_valid)\n",
    "\n",
    "    # Initialize target encoder and imputer\n",
    "    target_enc = TargetEncoder(cols=x_train.columns, handle_unknown='value', handle_missing='value')\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=-1)\n",
    "\n",
    "    # Apply target encoding\n",
    "    x_train = target_enc.fit_transform(x_train, y_train)\n",
    "    x_valid = target_enc.transform(x_valid)\n",
    "    x_train = imputer.fit_transform(x_train)\n",
    "    x_valid = imputer.transform(x_valid)\n",
    "\n",
    "    # Create XGBoost datasets\n",
    "    train_data = xgb.DMatrix(x_train, label=y_train)\n",
    "    valid_data = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "    # Train model\n",
    "    model = xgb.train(CFG.xgb_params, train_data, num_boost_round=CFG.num_boost_round, evals=[(train_data, 'train'), (valid_data, 'eval')], early_stopping_rounds=CFG.early_stopping_round, verbose_eval=CFG.verbose)\n",
    "\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(valid_data)\n",
    "    return model, valid_pred, one_hot_enc, target_enc\n",
    "\n",
    "def catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # Define model\n",
    "    model = CatBoostRegressor(**CFG.cat_params, cat_features=categorical_features)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], early_stopping_rounds=CFG.early_stopping_round, verbose=CFG.verbose)\n",
    "\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "\n",
    "    return model, valid_pred\n",
    "\n",
    "def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train_df))\n",
    "    oof_fold = np.zeros(len(train_df))\n",
    "    kfold = KFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    encoders = {}\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(train_df, train_df[CFG.target_col])):\n",
    "        print('-'*50)\n",
    "        print(f'{method} training fold {fold + 1}')\n",
    "        x_train = train_df[features].iloc[train_index]\n",
    "        y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "        x_valid = train_df[features].iloc[valid_index]\n",
    "        y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "        if method == 'lightgbm':\n",
    "            model, valid_pred, one_hot_enc, target_enc = lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            encoders[fold] = (one_hot_enc, target_enc)\n",
    "        if method == 'xgboost':\n",
    "            model, valid_pred, one_hot_enc, target_enc = xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            encoders[fold] = (one_hot_enc, target_enc)\n",
    "        if method == 'catboost':\n",
    "            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "\n",
    "        # Save best model\n",
    "        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "        # Save encoders only for lightgbm and xgboost\n",
    "        if method in ['lightgbm', 'xgboost']:\n",
    "            pickle.dump(encoders[fold], open(CFG.MODEL_DATA_PATH / f'{method}_encoders_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[valid_index] = valid_pred\n",
    "        oof_fold[valid_index] = fold + 1\n",
    "        del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute out of folds metric\n",
    "    score = np.sqrt(mean_squared_error(train_df[CFG.target_col], oof_predictions))\n",
    "    print(f'{method} our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'id': train_df['id'], CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "    oof_df.to_csv(CFG.MODEL_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CFG.DATA_PATH / 'train.csv')\n",
    "venue_info_df = pd.read_csv(CFG.DATA_PATH / 'venue_information.csv')\n",
    "test_df = pd.read_csv(CFG.DATA_PATH / 'test.csv')\n",
    "test_df[CFG.target_col] = -1\n",
    "match_reports_df = pd.read_csv('match_reports.csv')\n",
    "holidays_in_japan_df = pd.read_csv('holidays_in_japan.csv')\n",
    "submission_df = pd.read_csv(CFG.DATA_PATH / 'sample_submit.csv')\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_reports_df を 'id' カラムで all_df と結合します\n",
    "all_df = pd.merge(all_df, match_reports_df, on='id', how='left')\n",
    "\n",
    "# venue_info_df を 'venue' カラムで all_df と結合します\n",
    "all_df = pd.merge(all_df, venue_info_df, on='venue', how='left')\n",
    "\n",
    "# holidays_in_japan_df を 'match_date' カラムで all_df と結合します\n",
    "all_df['match_date'] = pd.to_datetime(all_df['match_date'])\n",
    "holidays_in_japan_df['holiday_date'] = pd.to_datetime(holidays_in_japan_df['holiday_date'])\n",
    "all_df['match_date'] = all_df['match_date'].dt.date\n",
    "holidays_in_japan_df['holiday_date'] = holidays_in_japan_df['holiday_date'].dt.date\n",
    "\n",
    "# もう一度 datetime 型に戻します\n",
    "all_df['match_date'] = pd.to_datetime(all_df['match_date'])\n",
    "holidays_in_japan_df['holiday_date'] = pd.to_datetime(holidays_in_japan_df['holiday_date'])\n",
    "\n",
    "all_df = pd.merge(all_df, holidays_in_japan_df, left_on='match_date', right_on='holiday_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_engineering as fe\n",
    "all_df = fe.apply_feature_engineering(all_df)\n",
    "all_df = fe.process_periodic_features(all_df)\n",
    "all_df[\"prefecture\"] = all_df[\"address\"].apply(fe.extract_prefecture)\n",
    "\n",
    "all_df = all_df.drop(['venue', 'address', 'description', 'match_date', 'kick_off_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.USE_PLAYER_FEATURES:\n",
    "    for i in range(1, 12):\n",
    "        all_df = all_df.drop([f'home_team_player{i}'],axis=1)\n",
    "        all_df = all_df.drop([f'away_team_player{i}'],axis=1)\n",
    "\n",
    "# 最後に、訓練データとテストデータに再度分割します\n",
    "train_df = all_df[all_df['attendance'] != -1]\n",
    "test_df = all_df[all_df['attendance'] == -1]\n",
    "\n",
    "# 'Id'や'Target'といった特定のカラムを除外した全てのカラムを特徴量とする場合\n",
    "features = train_df.columns.drop(['id', 'attendance'])\n",
    "\n",
    "# または、データ型が 'object'（文字列）または 'category' のカラムをカテゴリカル特徴量とする場合\n",
    "categorical_features = train_df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 936\n",
      "[LightGBM] [Info] Number of data points in the train set: 2692, number of used features: 113\n",
      "[LightGBM] [Info] Start training from score 18000.601783\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2000]\tvalid_0's l2: 2.04327e+07\n",
      "[4000]\tvalid_0's l2: 1.893e+07\n",
      "[6000]\tvalid_0's l2: 1.79546e+07\n",
      "[8000]\tvalid_0's l2: 1.74196e+07\n",
      "[10000]\tvalid_0's l2: 1.71219e+07\n",
      "[12000]\tvalid_0's l2: 1.68539e+07\n",
      "[14000]\tvalid_0's l2: 1.66757e+07\n",
      "[16000]\tvalid_0's l2: 1.65276e+07\n",
      "Early stopping, best iteration is:\n",
      "[17020]\tvalid_0's l2: 1.64614e+07\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 941\n",
      "[LightGBM] [Info] Number of data points in the train set: 2693, number of used features: 113\n",
      "[LightGBM] [Info] Start training from score 17849.561827\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2000]\tvalid_0's l2: 2.12597e+07\n",
      "[4000]\tvalid_0's l2: 1.96169e+07\n",
      "[6000]\tvalid_0's l2: 1.86859e+07\n",
      "[8000]\tvalid_0's l2: 1.80109e+07\n",
      "[10000]\tvalid_0's l2: 1.76411e+07\n",
      "[12000]\tvalid_0's l2: 1.73673e+07\n",
      "[14000]\tvalid_0's l2: 1.71449e+07\n",
      "[16000]\tvalid_0's l2: 1.69578e+07\n",
      "[18000]\tvalid_0's l2: 1.68736e+07\n",
      "[20000]\tvalid_0's l2: 1.67758e+07\n",
      "[22000]\tvalid_0's l2: 1.66789e+07\n",
      "[24000]\tvalid_0's l2: 1.66207e+07\n",
      "Early stopping, best iteration is:\n",
      "[24974]\tvalid_0's l2: 1.65863e+07\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 932\n",
      "[LightGBM] [Info] Number of data points in the train set: 2693, number of used features: 111\n",
      "[LightGBM] [Info] Start training from score 17812.410694\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2000]\tvalid_0's l2: 2.03166e+07\n",
      "[4000]\tvalid_0's l2: 1.9031e+07\n",
      "[6000]\tvalid_0's l2: 1.84036e+07\n",
      "[8000]\tvalid_0's l2: 1.80003e+07\n",
      "Early stopping, best iteration is:\n",
      "[8370]\tvalid_0's l2: 1.79846e+07\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 929\n",
      "[LightGBM] [Info] Number of data points in the train set: 2693, number of used features: 110\n",
      "[LightGBM] [Info] Start training from score 17950.524694\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2000]\tvalid_0's l2: 2.4082e+07\n",
      "[4000]\tvalid_0's l2: 2.30004e+07\n",
      "[6000]\tvalid_0's l2: 2.22524e+07\n",
      "[8000]\tvalid_0's l2: 2.19177e+07\n",
      "[10000]\tvalid_0's l2: 2.17209e+07\n",
      "[12000]\tvalid_0's l2: 2.15795e+07\n",
      "Early stopping, best iteration is:\n",
      "[11720]\tvalid_0's l2: 2.15542e+07\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 2693, number of used features: 110\n",
      "[LightGBM] [Info] Start training from score 18192.477163\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[2000]\tvalid_0's l2: 2.43146e+07\n",
      "[4000]\tvalid_0's l2: 2.30041e+07\n",
      "[6000]\tvalid_0's l2: 2.22924e+07\n",
      "[8000]\tvalid_0's l2: 2.19454e+07\n",
      "[10000]\tvalid_0's l2: 2.17027e+07\n",
      "[12000]\tvalid_0's l2: 2.15136e+07\n",
      "Early stopping, best iteration is:\n",
      "[12200]\tvalid_0's l2: 2.14859e+07\n",
      "lightgbm our out of folds CV score is 4337.48534882579\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1\n",
      "[0]\ttrain-rmse:20317.56521\teval-rmse:19858.83709\n",
      "[2000]\ttrain-rmse:3986.31949\teval-rmse:4513.58799\n",
      "[4000]\ttrain-rmse:3545.98913\teval-rmse:4314.07827\n",
      "[6000]\ttrain-rmse:3261.33257\teval-rmse:4208.98670\n",
      "[8000]\ttrain-rmse:3028.13333\teval-rmse:4144.88245\n",
      "[10000]\ttrain-rmse:2828.74155\teval-rmse:4106.95519\n",
      "[12000]\ttrain-rmse:2653.55711\teval-rmse:4082.36677\n",
      "[14000]\ttrain-rmse:2499.49787\teval-rmse:4064.15605\n",
      "[16000]\ttrain-rmse:2359.52656\teval-rmse:4056.24644\n",
      "[18000]\ttrain-rmse:2233.80251\teval-rmse:4047.54144\n",
      "[19590]\ttrain-rmse:2141.02589\teval-rmse:4046.16721\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2\n",
      "[0]\ttrain-rmse:20121.15654\teval-rmse:20644.01083\n",
      "[2000]\ttrain-rmse:4013.52675\teval-rmse:4615.26293\n",
      "[4000]\ttrain-rmse:3591.96791\teval-rmse:4369.65915\n",
      "[6000]\ttrain-rmse:3312.90403\teval-rmse:4256.46643\n",
      "[8000]\ttrain-rmse:3085.94171\teval-rmse:4185.85240\n",
      "[10000]\ttrain-rmse:2886.79877\teval-rmse:4136.78449\n",
      "[12000]\ttrain-rmse:2711.38671\teval-rmse:4099.57411\n",
      "[14000]\ttrain-rmse:2558.21263\teval-rmse:4071.31942\n",
      "[16000]\ttrain-rmse:2421.14628\teval-rmse:4052.67942\n",
      "[18000]\ttrain-rmse:2296.31280\teval-rmse:4039.37743\n",
      "[20000]\ttrain-rmse:2182.61041\teval-rmse:4028.67575\n",
      "[22000]\ttrain-rmse:2075.74671\teval-rmse:4019.31945\n",
      "[24000]\ttrain-rmse:1978.41067\teval-rmse:4013.30712\n",
      "[25068]\ttrain-rmse:1929.64710\teval-rmse:4012.02833\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3\n",
      "[0]\ttrain-rmse:20023.39999\teval-rmse:21020.50684\n",
      "[2000]\ttrain-rmse:3996.58567\teval-rmse:4635.60710\n",
      "[4000]\ttrain-rmse:3560.93013\teval-rmse:4421.57922\n",
      "[6000]\ttrain-rmse:3288.13393\teval-rmse:4338.11533\n",
      "[8000]\ttrain-rmse:3061.76750\teval-rmse:4286.60831\n",
      "[10000]\ttrain-rmse:2862.25510\teval-rmse:4251.48456\n",
      "[12000]\ttrain-rmse:2687.39229\teval-rmse:4226.72069\n",
      "[14000]\ttrain-rmse:2530.23796\teval-rmse:4204.03540\n",
      "[16000]\ttrain-rmse:2388.67018\teval-rmse:4192.27891\n",
      "[18000]\ttrain-rmse:2258.78405\teval-rmse:4185.40170\n",
      "[20000]\ttrain-rmse:2139.52501\teval-rmse:4180.78517\n",
      "[22000]\ttrain-rmse:2029.95484\teval-rmse:4174.02379\n",
      "[22744]\ttrain-rmse:1992.13378\teval-rmse:4173.66376\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4\n",
      "[0]\ttrain-rmse:20198.86387\teval-rmse:20338.89894\n",
      "[2000]\ttrain-rmse:3992.42746\teval-rmse:4892.62456\n",
      "[4000]\ttrain-rmse:3569.86624\teval-rmse:4710.68573\n",
      "[6000]\ttrain-rmse:3297.77005\teval-rmse:4634.45712\n",
      "[8000]\ttrain-rmse:3076.37770\teval-rmse:4593.40500\n",
      "[10000]\ttrain-rmse:2877.46866\teval-rmse:4571.12721\n",
      "[12000]\ttrain-rmse:2703.18761\teval-rmse:4554.45785\n",
      "[14000]\ttrain-rmse:2547.56263\teval-rmse:4544.78984\n",
      "[16000]\ttrain-rmse:2407.76890\teval-rmse:4539.23545\n",
      "[17008]\ttrain-rmse:2342.09470\teval-rmse:4539.77384\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5\n",
      "[0]\ttrain-rmse:20467.82250\teval-rmse:19222.08959\n",
      "[2000]\ttrain-rmse:3973.62313\teval-rmse:4896.01116\n",
      "[4000]\ttrain-rmse:3560.57074\teval-rmse:4708.77451\n",
      "[6000]\ttrain-rmse:3290.20100\teval-rmse:4629.87837\n",
      "[8000]\ttrain-rmse:3063.77230\teval-rmse:4582.32686\n",
      "[10000]\ttrain-rmse:2863.07450\teval-rmse:4547.91074\n",
      "[12000]\ttrain-rmse:2685.05440\teval-rmse:4529.11860\n",
      "[14000]\ttrain-rmse:2524.89597\teval-rmse:4512.12006\n",
      "[16000]\ttrain-rmse:2384.12601\teval-rmse:4500.50463\n",
      "[18000]\ttrain-rmse:2252.61355\teval-rmse:4491.99684\n",
      "[20000]\ttrain-rmse:2133.26640\teval-rmse:4487.40100\n",
      "[22000]\ttrain-rmse:2026.15949\teval-rmse:4481.41600\n",
      "[24000]\ttrain-rmse:1927.55252\teval-rmse:4477.31882\n",
      "[25617]\ttrain-rmse:1853.37657\teval-rmse:4475.10359\n",
      "xgboost our out of folds CV score is 4254.9140123220195\n",
      "--------------------------------------------------\n",
      "catboost training fold 1\n",
      "0:\tlearn: 9557.4514935\ttest: 8945.7837050\tbest: 8945.7837050 (0)\ttotal: 56.7ms\tremaining: 47m 42s\n",
      "2000:\tlearn: 4146.5571774\ttest: 4096.7296135\tbest: 4096.7296135 (2000)\ttotal: 1.55s\tremaining: 37.7s\n",
      "4000:\tlearn: 3661.0631268\ttest: 3940.2706802\tbest: 3940.2479105 (3996)\ttotal: 3.1s\tremaining: 36s\n",
      "6000:\tlearn: 3379.8965346\ttest: 3874.4895382\tbest: 3874.4895382 (6000)\ttotal: 4.92s\tremaining: 36.5s\n",
      "8000:\tlearn: 3159.2044044\ttest: 3835.9646961\tbest: 3835.9646961 (8000)\ttotal: 6.58s\tremaining: 35s\n",
      "10000:\tlearn: 2970.8371514\ttest: 3799.7838117\tbest: 3799.7838117 (10000)\ttotal: 8.25s\tremaining: 33.4s\n",
      "12000:\tlearn: 2808.8357608\ttest: 3770.9522005\tbest: 3770.7093530 (11982)\ttotal: 10.1s\tremaining: 32.3s\n",
      "14000:\tlearn: 2668.6485046\ttest: 3746.4528231\tbest: 3746.4528231 (14000)\ttotal: 11.8s\tremaining: 30.7s\n",
      "16000:\tlearn: 2540.6442033\ttest: 3725.1472743\tbest: 3725.1202260 (15998)\ttotal: 13.5s\tremaining: 29.2s\n",
      "18000:\tlearn: 2428.3886421\ttest: 3711.7610076\tbest: 3711.7315026 (17998)\ttotal: 15.3s\tremaining: 27.7s\n",
      "20000:\tlearn: 2325.2810684\ttest: 3702.4909765\tbest: 3702.1818400 (19889)\ttotal: 17.1s\tremaining: 26.1s\n",
      "22000:\tlearn: 2229.5084056\ttest: 3692.2312164\tbest: 3692.1785451 (21927)\ttotal: 18.9s\tremaining: 24.4s\n",
      "24000:\tlearn: 2140.8286940\ttest: 3683.9253705\tbest: 3683.9058429 (23995)\ttotal: 20.6s\tremaining: 22.7s\n",
      "26000:\tlearn: 2063.2365065\ttest: 3679.8059641\tbest: 3679.2676065 (25867)\ttotal: 22.4s\tremaining: 21.1s\n",
      "28000:\tlearn: 1991.4552871\ttest: 3673.6892919\tbest: 3673.3735725 (27804)\ttotal: 24.1s\tremaining: 19.4s\n",
      "30000:\tlearn: 1925.1924742\ttest: 3668.7971742\tbest: 3668.4449138 (29621)\ttotal: 25.9s\tremaining: 17.7s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 3668.444914\n",
      "bestIteration = 29621\n",
      "\n",
      "Shrink model to first 29622 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2\n",
      "0:\tlearn: 9427.2761816\ttest: 9502.5361010\tbest: 9502.5361010 (0)\ttotal: 1.97ms\tremaining: 1m 39s\n",
      "2000:\tlearn: 4200.7209150\ttest: 4436.2319433\tbest: 4436.2319433 (2000)\ttotal: 1.61s\tremaining: 39.1s\n",
      "4000:\tlearn: 3759.1315424\ttest: 4169.8480571\tbest: 4169.8480571 (4000)\ttotal: 3.2s\tremaining: 37.2s\n",
      "6000:\tlearn: 3464.8114503\ttest: 4034.8705332\tbest: 4034.8705332 (6000)\ttotal: 4.88s\tremaining: 36.2s\n",
      "8000:\tlearn: 3229.9942840\ttest: 3944.8207050\tbest: 3944.8207050 (8000)\ttotal: 6.5s\tremaining: 34.5s\n",
      "10000:\tlearn: 3030.2179866\ttest: 3883.0353671\tbest: 3883.0353671 (10000)\ttotal: 8.13s\tremaining: 32.9s\n",
      "12000:\tlearn: 2868.4805488\ttest: 3845.5320401\tbest: 3845.3808798 (11984)\ttotal: 9.77s\tremaining: 31.3s\n",
      "14000:\tlearn: 2723.1110329\ttest: 3816.6561706\tbest: 3816.6561706 (14000)\ttotal: 11.4s\tremaining: 29.8s\n",
      "16000:\tlearn: 2600.0819833\ttest: 3795.6234670\tbest: 3795.5909366 (15998)\ttotal: 13.1s\tremaining: 28.2s\n",
      "18000:\tlearn: 2482.8169012\ttest: 3773.5832991\tbest: 3773.5415490 (17987)\ttotal: 14.7s\tremaining: 26.6s\n",
      "20000:\tlearn: 2377.4809015\ttest: 3754.8683094\tbest: 3754.7801814 (19998)\ttotal: 16.4s\tremaining: 25.1s\n",
      "22000:\tlearn: 2283.1531340\ttest: 3740.0253286\tbest: 3740.0004651 (21989)\ttotal: 18.2s\tremaining: 23.6s\n",
      "24000:\tlearn: 2192.4486884\ttest: 3729.6525855\tbest: 3729.5816334 (23991)\ttotal: 19.9s\tremaining: 22s\n",
      "26000:\tlearn: 2108.9853615\ttest: 3719.2203803\tbest: 3719.1006646 (25996)\ttotal: 21.7s\tremaining: 20.4s\n",
      "28000:\tlearn: 2032.0670304\ttest: 3713.8320264\tbest: 3713.8311173 (27999)\ttotal: 23.4s\tremaining: 18.8s\n",
      "30000:\tlearn: 1960.4784465\ttest: 3706.4959193\tbest: 3706.4317367 (29998)\ttotal: 25.1s\tremaining: 17.2s\n",
      "32000:\tlearn: 1894.7161141\ttest: 3699.3784077\tbest: 3699.3411984 (31996)\ttotal: 26.8s\tremaining: 15.5s\n",
      "34000:\tlearn: 1833.5291997\ttest: 3691.4066187\tbest: 3691.4066187 (34000)\ttotal: 28.6s\tremaining: 13.9s\n",
      "36000:\tlearn: 1774.8127464\ttest: 3687.3685245\tbest: 3687.2779792 (35887)\ttotal: 30.3s\tremaining: 12.2s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 3684.751791\n",
      "bestIteration = 36983\n",
      "\n",
      "Shrink model to first 36984 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3\n",
      "0:\tlearn: 9289.4367017\ttest: 10038.1209785\tbest: 10038.1209785 (0)\ttotal: 2.04ms\tremaining: 1m 43s\n",
      "2000:\tlearn: 4190.6449995\ttest: 4295.3543917\tbest: 4295.3543917 (2000)\ttotal: 1.54s\tremaining: 37.4s\n",
      "4000:\tlearn: 3735.5099638\ttest: 4063.9428429\tbest: 4063.9428429 (4000)\ttotal: 3.17s\tremaining: 36.9s\n",
      "6000:\tlearn: 3452.8314937\ttest: 3973.6683365\tbest: 3973.6066281 (5993)\ttotal: 4.82s\tremaining: 35.7s\n",
      "8000:\tlearn: 3212.2076646\ttest: 3897.7294503\tbest: 3897.7294503 (8000)\ttotal: 6.48s\tremaining: 34.4s\n",
      "10000:\tlearn: 3015.0419479\ttest: 3850.6559722\tbest: 3850.5788260 (9993)\ttotal: 8.16s\tremaining: 33s\n",
      "12000:\tlearn: 2853.7900873\ttest: 3812.6225213\tbest: 3812.5915614 (11998)\ttotal: 9.82s\tremaining: 31.5s\n",
      "14000:\tlearn: 2710.0077512\ttest: 3782.9999450\tbest: 3782.9787954 (13965)\ttotal: 11.6s\tremaining: 30.2s\n",
      "16000:\tlearn: 2582.3001511\ttest: 3758.4346850\tbest: 3758.3773225 (15995)\ttotal: 13.3s\tremaining: 28.7s\n",
      "18000:\tlearn: 2469.8345281\ttest: 3741.6146822\tbest: 3741.5487416 (17997)\ttotal: 15.1s\tremaining: 27.2s\n",
      "20000:\tlearn: 2366.9499895\ttest: 3730.8241331\tbest: 3730.8241331 (20000)\ttotal: 16.7s\tremaining: 25.5s\n",
      "22000:\tlearn: 2273.7592466\ttest: 3723.5248146\tbest: 3723.3018503 (21978)\ttotal: 18.4s\tremaining: 23.8s\n",
      "24000:\tlearn: 2187.2926775\ttest: 3717.5371611\tbest: 3717.4843769 (23991)\ttotal: 20.1s\tremaining: 22.2s\n",
      "26000:\tlearn: 2107.1926120\ttest: 3712.6004580\tbest: 3712.2598881 (25900)\ttotal: 21.8s\tremaining: 20.5s\n",
      "28000:\tlearn: 2031.9724460\ttest: 3709.4483678\tbest: 3709.2378380 (27697)\ttotal: 23.5s\tremaining: 18.9s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 3709.237838\n",
      "bestIteration = 27697\n",
      "\n",
      "Shrink model to first 27698 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4\n",
      "0:\tlearn: 9401.6451729\ttest: 9601.5345459\tbest: 9601.5345459 (0)\ttotal: 1.58ms\tremaining: 1m 19s\n",
      "2000:\tlearn: 4211.6632693\ttest: 4389.0855868\tbest: 4389.0855868 (2000)\ttotal: 1.5s\tremaining: 36.4s\n",
      "4000:\tlearn: 3762.0341092\ttest: 4148.5562079\tbest: 4148.5562079 (4000)\ttotal: 3.11s\tremaining: 36.1s\n",
      "6000:\tlearn: 3435.3494951\ttest: 4016.8439278\tbest: 4016.6257114 (5994)\ttotal: 4.75s\tremaining: 35.3s\n",
      "8000:\tlearn: 3186.2862552\ttest: 3919.6821712\tbest: 3919.6821712 (8000)\ttotal: 6.36s\tremaining: 33.8s\n",
      "10000:\tlearn: 2989.3168570\ttest: 3856.8506880\tbest: 3856.8506880 (10000)\ttotal: 8.02s\tremaining: 32.5s\n",
      "12000:\tlearn: 2825.8788593\ttest: 3808.1848466\tbest: 3808.1848466 (12000)\ttotal: 9.67s\tremaining: 31s\n",
      "14000:\tlearn: 2685.1479989\ttest: 3776.0587912\tbest: 3775.9998511 (13995)\ttotal: 11.3s\tremaining: 29.6s\n",
      "16000:\tlearn: 2561.5055402\ttest: 3750.3671786\tbest: 3750.3331250 (15998)\ttotal: 13s\tremaining: 28s\n",
      "18000:\tlearn: 2450.0322602\ttest: 3728.0028567\tbest: 3727.9439539 (17996)\ttotal: 14.8s\tremaining: 26.8s\n",
      "20000:\tlearn: 2349.5452990\ttest: 3711.8900169\tbest: 3711.8754412 (19999)\ttotal: 16.7s\tremaining: 25.4s\n",
      "22000:\tlearn: 2256.7125025\ttest: 3696.2748749\tbest: 3696.2748749 (22000)\ttotal: 18.4s\tremaining: 23.8s\n",
      "24000:\tlearn: 2168.6679752\ttest: 3685.8429758\tbest: 3685.6871663 (23984)\ttotal: 20s\tremaining: 22.1s\n",
      "26000:\tlearn: 2088.3853869\ttest: 3676.4901070\tbest: 3676.3158141 (25970)\ttotal: 21.6s\tremaining: 20.4s\n",
      "28000:\tlearn: 2014.7506588\ttest: 3668.2668054\tbest: 3668.2264092 (27957)\ttotal: 23.3s\tremaining: 18.8s\n",
      "30000:\tlearn: 1945.2045420\ttest: 3659.4439151\tbest: 3659.4310338 (29997)\ttotal: 25.1s\tremaining: 17.1s\n",
      "32000:\tlearn: 1880.3765808\ttest: 3654.0129217\tbest: 3654.0129217 (32000)\ttotal: 26.8s\tremaining: 15.5s\n",
      "34000:\tlearn: 1822.5538631\ttest: 3649.1781150\tbest: 3649.0969873 (33992)\ttotal: 28.5s\tremaining: 13.8s\n",
      "36000:\tlearn: 1765.8448318\ttest: 3645.4741316\tbest: 3645.4276930 (35995)\ttotal: 30.2s\tremaining: 12.2s\n",
      "38000:\tlearn: 1711.3018880\ttest: 3641.6554035\tbest: 3641.6340722 (37977)\ttotal: 32s\tremaining: 10.5s\n",
      "40000:\tlearn: 1660.1909380\ttest: 3638.6967555\tbest: 3638.6845758 (39602)\ttotal: 33.7s\tremaining: 8.85s\n",
      "42000:\tlearn: 1612.2632926\ttest: 3636.1399253\tbest: 3635.7097335 (41553)\ttotal: 35.4s\tremaining: 7.17s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 3635.709734\n",
      "bestIteration = 41553\n",
      "\n",
      "Shrink model to first 41554 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5\n",
      "0:\tlearn: 9524.7454828\ttest: 9123.0545561\tbest: 9123.0545561 (0)\ttotal: 1.64ms\tremaining: 1m 23s\n",
      "2000:\tlearn: 4128.8501667\ttest: 4579.4443950\tbest: 4579.4217928 (1999)\ttotal: 1.55s\tremaining: 37.6s\n",
      "4000:\tlearn: 3630.6158828\ttest: 4360.8654847\tbest: 4360.6144818 (3997)\ttotal: 3.15s\tremaining: 36.6s\n",
      "6000:\tlearn: 3307.8639471\ttest: 4255.3576998\tbest: 4255.3576998 (6000)\ttotal: 4.78s\tremaining: 35.4s\n",
      "8000:\tlearn: 3084.3725798\ttest: 4198.7844598\tbest: 4198.7844598 (8000)\ttotal: 6.44s\tremaining: 34.2s\n",
      "10000:\tlearn: 2900.1688910\ttest: 4154.6349268\tbest: 4154.6075345 (9994)\ttotal: 8.12s\tremaining: 32.9s\n",
      "12000:\tlearn: 2741.3038930\ttest: 4118.8395432\tbest: 4118.8395432 (12000)\ttotal: 9.82s\tremaining: 31.5s\n",
      "14000:\tlearn: 2604.3195815\ttest: 4097.3119346\tbest: 4097.2946012 (13999)\ttotal: 11.5s\tremaining: 29.9s\n",
      "16000:\tlearn: 2482.8438860\ttest: 4079.8137115\tbest: 4079.8137115 (16000)\ttotal: 13.2s\tremaining: 28.4s\n",
      "18000:\tlearn: 2376.1837619\ttest: 4064.1494499\tbest: 4064.1494499 (18000)\ttotal: 14.8s\tremaining: 26.8s\n",
      "20000:\tlearn: 2281.1327311\ttest: 4052.4088364\tbest: 4052.1949182 (19987)\ttotal: 16.5s\tremaining: 25.2s\n",
      "22000:\tlearn: 2193.8992457\ttest: 4042.1065548\tbest: 4042.1065548 (22000)\ttotal: 18.2s\tremaining: 23.6s\n",
      "24000:\tlearn: 2115.1981151\ttest: 4036.2295367\tbest: 4036.1684552 (23992)\ttotal: 20s\tremaining: 22s\n",
      "26000:\tlearn: 2041.7553718\ttest: 4029.3556339\tbest: 4028.9133562 (25862)\ttotal: 21.7s\tremaining: 20.4s\n",
      "28000:\tlearn: 1973.6401391\ttest: 4022.9593005\tbest: 4022.8254947 (27986)\ttotal: 23.4s\tremaining: 18.8s\n",
      "30000:\tlearn: 1908.7874814\ttest: 4016.7728561\tbest: 4016.7016387 (29956)\ttotal: 25.3s\tremaining: 17.3s\n",
      "32000:\tlearn: 1845.8733342\ttest: 4012.0239852\tbest: 4011.6513069 (31862)\ttotal: 27.1s\tremaining: 15.7s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 4011.651307\n",
      "bestIteration = 31862\n",
      "\n",
      "Shrink model to first 31863 iterations.\n",
      "catboost our out of folds CV score is 3744.4418430243813\n"
     ]
    }
   ],
   "source": [
    "for method in CFG.METHOD_LIST:\n",
    "    gradient_boosting_model_cv_training(method, train_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_encoders(data: pd.DataFrame, one_hot_enc: OneHotEncoder, target_enc: TargetEncoder):\n",
    "  data = data.copy()\n",
    "\n",
    "  # Apply one-hot encoding\n",
    "  data = one_hot_enc.transform(data)\n",
    "\n",
    "  # Apply target encoding\n",
    "  data = target_enc.transform(data)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(CFG.MODEL_DATA_PATH / f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        one_hot_enc, target_enc = pickle.load(open(CFG.MODEL_DATA_PATH / f'lightgbm_encoders_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        x_test_enc = apply_encoders(x_test, one_hot_enc, target_enc)\n",
    "        test_pred += model.predict(x_test_enc)\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(CFG.MODEL_DATA_PATH / f'xgboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        one_hot_enc, target_enc = pickle.load(open(CFG.MODEL_DATA_PATH / f'xgboost_encoders_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        x_test_enc = apply_encoders(x_test, one_hot_enc, target_enc)\n",
    "        test_pred += model.predict(xgb.DMatrix(x_test_enc), iteration_range=(0, model.best_ntree_limit))\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(CFG.MODEL_DATA_PATH / f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        test_pred += model.predict(x_test)\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    x_test = test_df[features]\n",
    "    if method in ['lightgbm', 'xgboost']:\n",
    "        test_pred = lightgbm_inference(x_test) if method == 'lightgbm' else xgboost_inference(x_test)\n",
    "    if method == 'catboost':\n",
    "        test_pred = catboost_inference(x_test)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "for method in CFG.METHOD_LIST:\n",
    "    test_df[f'{method}_pred'] = gradient_boosting_model_inference(method, test_df, features, categorical_features)\n",
    "\n",
    "test_df['final_pred'] = 0.4 * test_df['lightgbm_pred'] + 0.2 * test_df['xgboost_pred'] + 0.4 * test_df['catboost_pred']\n",
    "\n",
    "# 結果を保存\n",
    "test_df[['id','final_pred']].to_csv('submission.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19075</th>\n",
       "      <th>37779.411026584334</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19076</td>\n",
       "      <td>18255.540461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19077</td>\n",
       "      <td>25724.500203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19078</td>\n",
       "      <td>17926.242928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19079</td>\n",
       "      <td>18044.249286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19080</td>\n",
       "      <td>14721.822971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>20893</td>\n",
       "      <td>28927.997498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>20894</td>\n",
       "      <td>20255.052594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>20895</td>\n",
       "      <td>26215.179150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>20896</td>\n",
       "      <td>28748.995518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>20897</td>\n",
       "      <td>11519.456949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     19075  37779.411026584334\n",
       "0    19076        18255.540461\n",
       "1    19077        25724.500203\n",
       "2    19078        17926.242928\n",
       "3    19079        18044.249286\n",
       "4    19080        14721.822971\n",
       "..     ...                 ...\n",
       "453  20893        28927.997498\n",
       "454  20894        20255.052594\n",
       "455  20895        26215.179150\n",
       "456  20896        28748.995518\n",
       "457  20897        11519.456949\n",
       "\n",
       "[458 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
