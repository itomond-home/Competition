{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: oof: File exists\n",
      "mkdir: models: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir oof\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DATA_PATH = Path('.')\n",
    "    OOF_DATA_PATH = Path('./oof')\n",
    "    MODEL_DATA_PATH = Path('./models')\n",
    "    METHOD_LIST = ['catboost', 'lightgbm']\n",
    "    seed = random.randint(0, 100)\n",
    "    n_folds = 10\n",
    "    target_col = 'attendance'\n",
    "    USE_PLAYER_FEATURES = False\n",
    "    num_boost_round = 50500\n",
    "    early_stopping_round = 500\n",
    "    verbose = 2000\n",
    "    boosting_type = 'dart'\n",
    "    lgb_params = {\n",
    "        'objective':'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting': 'dart',\n",
    "        'n_jobs': -1,\n",
    "        'seed': seed,\n",
    "        'feature_pre_filter': False,\n",
    "        'lambda_l1': 0.0,\n",
    "        'lambda_l2': 0.0,\n",
    "        'num_leaves': 4,\n",
    "        'feature_fraction': 0.9159999999999999,\n",
    "        'bagging_fraction': 0.808098964054993,\n",
    "        'bagging_freq': 1,\n",
    "        'min_child_samples': 20,\n",
    "        }\n",
    "    cat_params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'iterations': num_boost_round,\n",
    "        'random_seed': seed,\n",
    "        'thread_count': -1,\n",
    "        'depth': 5,\n",
    "        'learning_rate': 0.010809655967120722,\n",
    "        'random_strength': 83,\n",
    "        'bagging_temperature': 94.09902179801527,\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 49\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'rmse', np.sqrt(mean_squared_error(y_true, y_pred)), False\n",
    "\n",
    "# ====================================================\n",
    "# Catboost Metric\n",
    "# ====================================================\n",
    "class CatboostMetric(object):\n",
    "    def get_final_error(self, error, weight): return error\n",
    "    def is_max_optimal(self): return False\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        error = np.sqrt(mean_squared_error(np.array(target), approxes))\n",
    "        return error, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame):\n",
    "    # Load the encoders from a pickle file\n",
    "    with open('encoders.pkl', 'rb') as f:\n",
    "        encoders = pickle.load(f)\n",
    "\n",
    "    # Apply the encoding map to the relevant columns\n",
    "    categorical_cols = x_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    for fold in range(5):  # replace 5 with your number of folds\n",
    "        for col in categorical_cols:\n",
    "            x_train.loc[x_train['fold'] == fold, f'{col}_enc'] = x_train.loc[x_train['fold'] == fold, col].map(encoders[f'fold_{fold}_{col}'])\n",
    "            x_valid.loc[x_valid['fold'] == fold, f'{col}_enc'] = x_valid.loc[x_valid['fold'] == fold, col].map(encoders[f'fold_{fold}_{col}'])\n",
    "\n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(x_train, label=y_train, categorical_feature=categorical_cols)\n",
    "    valid_data = lgb.Dataset(x_valid, label=y_valid, categorical_feature=categorical_cols)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params=CFG.lgb_params,\n",
    "        train_set=train_data,\n",
    "        num_boost_round=20000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(CFG.early_stopping_round, verbose=True), lgb.log_evaluation(CFG.verbose)]\n",
    "        )\n",
    "\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "def catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, categorical_features: list):\n",
    "    # Define model\n",
    "    model = CatBoostRegressor(**CFG.cat_params, cat_features=categorical_features)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], early_stopping_rounds=CFG.early_stopping_round, verbose=CFG.verbose)\n",
    "\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "\n",
    "    return model, valid_pred\n",
    "\n",
    "def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train_df))\n",
    "    oof_fold = np.zeros(len(train_df))\n",
    "    kfold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(train_df, train_df[CFG.target_col])):\n",
    "        print('-'*50)\n",
    "        print(f'{method} training fold {fold + 1}')\n",
    "        x_train = train_df[features].iloc[train_index]\n",
    "        y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "        x_valid = train_df[features].iloc[valid_index]\n",
    "        y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "        if method == 'lightgbm':\n",
    "            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid)\n",
    "        if method == 'catboost':\n",
    "            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "\n",
    "        # Save best model\n",
    "        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.boosting_type}.pkl', 'wb'))\n",
    "        # Save encoders only for lightgbm and xgboost\n",
    "        # if method in ['lightgbm', 'xgboost']:\n",
    "        #     pickle.dump(encoders[fold], open(CFG.MODEL_DATA_PATH / f'{method}_encoders_fold{fold + 1}_seed{CFG.seed}_ver{CFG.boosting_type}.pkl', 'wb'))\n",
    "\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[valid_index] = valid_pred\n",
    "        oof_fold[valid_index] = fold + 1\n",
    "        del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute out of folds metric\n",
    "    score = np.sqrt(mean_squared_error(train_df[CFG.target_col], oof_predictions))\n",
    "    print(f'{method} our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    # oof_df = pd.DataFrame({'id': train_df['id'], CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "    # oof_df.to_csv(CFG.MODEL_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.boosting_type}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CFG.DATA_PATH / 'train.csv')\n",
    "venue_info_df = pd.read_csv(CFG.DATA_PATH / 'venue_information.csv')\n",
    "test_df = pd.read_csv(CFG.DATA_PATH / 'test.csv')\n",
    "test_df[CFG.target_col] = -1\n",
    "match_reports_df = pd.read_csv('match_reports.csv')\n",
    "holidays_in_japan_df = pd.read_csv('holidays_in_japan.csv')\n",
    "submission_df = pd.read_csv(CFG.DATA_PATH / 'sample_submit.csv')\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_reports_df を 'id' カラムで all_df と結合します\n",
    "all_df = pd.merge(all_df, match_reports_df, on='id', how='left')\n",
    "\n",
    "# venue_info_df を 'venue' カラムで all_df と結合します\n",
    "all_df = pd.merge(all_df, venue_info_df, on='venue', how='left')\n",
    "\n",
    "# holidays_in_japan_df を 'match_date' カラムで all_df と結合します\n",
    "all_df['match_date'] = pd.to_datetime(all_df['match_date'])\n",
    "holidays_in_japan_df['holiday_date'] = pd.to_datetime(holidays_in_japan_df['holiday_date'])\n",
    "all_df['match_date'] = all_df['match_date'].dt.date\n",
    "holidays_in_japan_df['holiday_date'] = holidays_in_japan_df['holiday_date'].dt.date\n",
    "\n",
    "# もう一度 datetime 型に戻します\n",
    "all_df['match_date'] = pd.to_datetime(all_df['match_date'])\n",
    "holidays_in_japan_df['holiday_date'] = pd.to_datetime(holidays_in_japan_df['holiday_date'])\n",
    "\n",
    "all_df = pd.merge(all_df, holidays_in_japan_df, left_on='match_date', right_on='holiday_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_engineering as fe\n",
    "\n",
    "all_df = fe.apply_feature_engineering(all_df)\n",
    "all_df = fe.process_periodic_features(all_df)\n",
    "all_df = fe.add_geographical_features(all_df, venue_info_df)\n",
    "all_df = fe.add_grouped_statistics(all_df)\n",
    "all_df = all_df.dropna()\n",
    "all_df = fe.HDBSCAN_featuring(all_df)\n",
    "all_df = all_df.drop(['venue', 'address', 'description', 'match_date', 'kick_off_time'], axis=1)\n",
    "\n",
    "# 最後に、訓練データとテストデータに再度分割します\n",
    "train_df = all_df[all_df['attendance'] != -1].dropna()\n",
    "test_df = all_df[all_df['attendance'] == -1]\n",
    "\n",
    "all_df = fe.compute_knn_features_and_preprocess(train_df, test_df, CFG.target_col, k=3, folds=5)\n",
    "\n",
    "# 最後に、訓練データとテストデータに再度分割します\n",
    "train_df = all_df[all_df['attendance'] != -1].dropna()\n",
    "test_df = all_df[all_df['attendance'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 257 entries, 0 to 256\n",
      "Columns: 133 entries, id to knn_avg_dist_3\n",
      "dtypes: bool(4), float64(73), int32(5), int64(46), object(5)\n",
      "memory usage: 257.0+ KB\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, section, round, home_team, away_team, weather, temperature, humidity, attendance, home_team_score, away_team_score, capacity, スカパー!, スカパー!プレミアムサービス, スカパー, e2, スカパー(スカチャンHD、スカチャン), DAZN, スカパー!(パーフェクト チョイス), スカパー光, e2スカチャン, J SPORTS(録), NHK BS1, e2スカチャンHD, BS, e2(スカチャン!), テレ玉, BS-i, BS-TBS, e2(スカチャン!HV), 静岡放送, NHK総合, holiday_flag, long_weekend_flag, discomfort_index, home_team_rank, away_team_rank, home_team_last_year_rank, away_team_last_year_rank, rank_diff, rank_diff_abs, last_year_rank_diff, last_year_rank_diff_abs, diff_score, home_team_avg_conceded_last_3, away_team_avg_conceded_last_3, home_team_scored, away_team_scored, home_team_conceded, away_team_conceded, home_team_avg_points_last_3, home_team_avg_points_last_5, away_team_avg_points_last_3, away_team_avg_points_last_5, home_team_avg_scored_last_3, home_team_avg_scored_last_5, away_team_avg_scored_last_3, away_team_avg_scored_last_5, home_team_winning_streak, away_team_winning_streak, home_team_losing_streak, away_team_losing_streak, most_freq_home_player_in_match, second_most_freq_home_player_in_match, most_freq_away_player_in_match, second_most_freq_away_player_in_match, year, month, day, day_of_week, hour, month_sin, month_cos, day_sin, day_cos, day_of_week_sin, day_of_week_cos, hour_sin, hour_cos, section_sin, section_cos, venue_prefecture, venue_region, away_prefecture, home_same_prefecture, home_same_region, away_same_prefecture, away_same_region, distance_category, temperature_max_by_venue, temperature_min_by_venue, temperature_var_by_venue, temperature_mean_by_venue, temperature_sum_by_venue, discomfort_index_max_by_venue, discomfort_index_min_by_venue, discomfort_index_var_by_venue, discomfort_index_mean_by_venue, discomfort_index_sum_by_venue, humidity_max_by_venue, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "missing_rows = all_df[all_df.isnull().any(axis=1)]\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smooth_mean(df, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = df[on].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean\n",
    "    return df[by].map(smooth)\n",
    "\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=CFG.seed)\n",
    "encoders = {}  # to save the encoding map for each fold\n",
    "\n",
    "train_df['fold'] = -1  # Create a new column to store the fold number\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n",
    "    train_data = train_df.loc[train_idx]\n",
    "    valid_data = train_df.loc[valid_idx]\n",
    "\n",
    "    valid_data['fold'] = fold  # Assign the fold number to the valid_data fold column\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        valid_data[f'{col}_enc'] = calc_smooth_mean(train_data, by=col, on=CFG.target_col, m=300)\n",
    "        encoders[f'fold_{fold}_{col}'] = valid_data[f'{col}_enc'].to_dict()  # save the encoding map\n",
    "\n",
    "    train_df.loc[valid_idx] = valid_data\n",
    "\n",
    "# Save the encoders to a pickle file\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns in X: ['home_team', 'away_team', 'venue_prefecture', 'venue_region', 'away_prefecture']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '大分'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodel_tuner\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df \u001b[39m=\u001b[39m model_tuner\u001b[39m.\u001b[39;49mselect_features_and_tune_parameters(train_df, \u001b[39m'\u001b[39;49m\u001b[39mattendance\u001b[39;49m\u001b[39m'\u001b[39;49m, CFG\u001b[39m.\u001b[39;49mseed)\n",
      "File \u001b[0;32m~/Documents/GitHub/Competition/Deloitte2023/model_tuner.py:100\u001b[0m, in \u001b[0;36mselect_features_and_tune_parameters\u001b[0;34m(train_df, target_col, seed)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m# Perform Recursive Feature Elimination using RFECV\u001b[39;00m\n\u001b[1;32m     99\u001b[0m rfecv \u001b[39m=\u001b[39m RFECV(estimator\u001b[39m=\u001b[39mgbm, step\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39mKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39mseed), scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_root_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m, min_features_to_select\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m rfecv\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m    102\u001b[0m selected_features \u001b[39m=\u001b[39m features \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattendance\u001b[39m\u001b[39m'\u001b[39m])[rfecv\u001b[39m.\u001b[39msupport_]\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(selected_features)\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:678\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m    677\u001b[0m tags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()\n\u001b[0;32m--> 678\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    679\u001b[0m     X,\n\u001b[1;32m    680\u001b[0m     y,\n\u001b[1;32m    681\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    682\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m    683\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tags\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mallow_nan\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m    684\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[39m# Initialization\u001b[39;00m\n\u001b[1;32m    688\u001b[0m cv \u001b[39m=\u001b[39m check_cv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv, y, classifier\u001b[39m=\u001b[39mis_classifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator))\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/sklearn/utils/validation.py:810\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    806\u001b[0m     \u001b[39m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# nans\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[39m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     new_dtype \u001b[39m=\u001b[39m dtype_orig \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dtype\n\u001b[0;32m--> 810\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39;49mastype(new_dtype)\n\u001b[1;32m    811\u001b[0m     \u001b[39m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    452\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    454\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    455\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    456\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[1;32m    457\u001b[0m )\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    513\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:187\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    184\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    189\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.anyenv/envs/pyenv/versions/miniforge3-22.11.1-4/envs/deloitte2023/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:138\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m    137\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '大分'"
     ]
    }
   ],
   "source": [
    "import model_tuner\n",
    "train_df = model_tuner.select_features_and_tune_parameters(train_df, 'attendance', CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Id'や'Target'といった特定のカラムを除外した全てのカラムを特徴量とする場合\n",
    "features = train_df.columns.drop(['id', 'attendance'])\n",
    "\n",
    "# または、データ型が 'object'（文字列）または 'category' のカラムをカテゴリカル特徴量とする場合\n",
    "categorical_features = train_df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング\n",
    "CFG.cat_params = model_tuner.tune_model(train_df[features], train_df[CFG.target_col], n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in CFG.METHOD_LIST:\n",
    "    gradient_boosting_model_cv_training(method, train_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_inference(x_test: pd.DataFrame):\n",
    "    # Load the encoders from a pickle file\n",
    "    with open('encoders.pkl', 'rb') as f:\n",
    "        encoders = pickle.load(f)\n",
    "\n",
    "    # Apply the encoding map to the relevant columns\n",
    "    categorical_cols = x_test.select_dtypes(include=['object', 'category']).columns\n",
    "    for fold in range(CFG.n_folds):\n",
    "        for col in categorical_cols:\n",
    "            x_test[f'{col}_enc'] = x_test[col].map(encoders[f'fold_{fold}_{col}'])\n",
    "\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(CFG.MODEL_DATA_PATH / f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.boosting_type}.pkl', 'rb'))\n",
    "        categorical_cols = x_test.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in categorical_cols:\n",
    "            x_test[col] = x_test[col].astype('category')\n",
    "        test_pred += model.predict(x_test)\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(CFG.MODEL_DATA_PATH / f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.boosting_type}.pkl', 'rb'))\n",
    "        test_pred += model.predict(x_test)\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list):\n",
    "    x_test = test_df[features]\n",
    "    if method == 'lightgbm':\n",
    "        test_pred = lightgbm_inference(x_test)\n",
    "    if method == 'catboost':\n",
    "        test_pred = catboost_inference(x_test)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "for method in CFG.METHOD_LIST:\n",
    "    test_df[f'{method}_pred'] = gradient_boosting_model_inference(method, test_df, features)\n",
    "\n",
    "\n",
    "# アンサンブルの結果を保存\n",
    "test_df['final_pred'] = 0.5 * test_df['lightgbm_pred'] + 0.5 * test_df['catboost_pred']\n",
    "test_df[['id','final_pred']].to_csv(f'submissions/EMS_{CFG.n_folds}folds_submission_{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{CFG.seed}.csv', index=False, header=False)\n",
    "\n",
    "# LGBのみの結果を保存\n",
    "test_df['final_pred'] = 1.0 * test_df['lightgbm_pred'] + 0.0 * test_df['catboost_pred']\n",
    "test_df[['id','final_pred']].to_csv(f'submissions/OnlyLGB_{CFG.n_folds}folds_submission_{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{CFG.seed}.csv', index=False, header=False)\n",
    "\n",
    "# Catのみの結果を保存\n",
    "test_df['final_pred'] = 0.0 * test_df['lightgbm_pred'] + 1.0 * test_df['catboost_pred']\n",
    "test_df[['id','final_pred']].to_csv(f'submissions/OnlyCat_{CFG.n_folds}folds_submission_{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{CFG.seed}.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
